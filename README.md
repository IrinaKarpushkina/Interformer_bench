# Подробности нашей работы с Interformer

Основной скрипт, который запускает работу пайплайна - **```pipeline.sh```**. Старались там тоже подробно комментировать каждый этап.

### 1. Как мы готовили белоки и лиганды
Белок брали предварительно обработанный meeko (больше ничего с ним не делали).
Референсный лиганд протонировали, как описано в оригинальном REDME:

``` obabel ${WORK_PATH}/raw/${PDB}_ligand.pdb -p 7.4 -O ${WORK_PATH}/ligand/${PDB}_docked.sdf ```

Для подготовки исследуемых лигандов нами был написан скрипт **```prepare_ligands.py```**. 
**Основная задача скрипта** — чтение молекулярных структур из CSV-файла, их преобразование, фильтрация и сохранение в формате SDF.

#### Ключевые функции **```prepare_ligands.py```**:

* **Фильтрация по молекулярной массе:** Проводится проверка молекулярной массы каждой молекулы. Молекулы с массой, превышающей заданный порог (по умолчанию 500 Да), отфильтровываются и не проходят дальнейшую обработку.
*   К молекуле добавляются атомы водорода (`Chem.AddHs`).
*   Генерируются 3D-координаты атомов с использованием алгоритма ETKDGv3 (`AllChem.EmbedMolecule`).
*   Проводится оптимизация геометрии молекулы с помощью силового поля UFF (`AllChem.UFFOptimizeMolecule`) для минимизации ее энергии.
*   Подготовленные 3D-структуры молекул сохраняются в выходной файл в формате SDF (Structure-Data File). 
*   Каждой сохраненной молекуле присваивается имя, соответствующее номеру исходной строки в CSV-файле.


### 2. Внутренняя обработка белков и лигандов Interformer

#### Общая схема подготовки

#### Шаг 1: Подготовка Белка (Protein Preparation)

Основная задача здесь — выделить из всей огромной структуры белка только ту его часть, которая взаимодействует с лигандом (карман связывания), и подготовить ее.

#### 1.1. Добавление водородов и определение состояний протонирования

*   **Что происходит:** На этом этапе к структуре белка добавляются атомы водорода, так как в большинстве PDB-файлов они отсутствуют. Также определяется, какие аминокислотные остатки (например, гистидин, аспартат, глутамат) должны быть протонированы (нести заряд) при физиологическом pH.
*   **Инструмент:** В README упоминается программа **Reduce**. Это стандартный инструмент в биоинформатике для этой задачи.
    ```bash
    # Команда из README
    reduce examples/raw/2qbr.pdb > examples/raw/pocket/2qbr_reduce.pdb
    ```
*   **Результат:** Новый PDB-файл (`2qbr_reduce.pdb`), содержащий полную структуру белка с правильно расставленными атомами водорода.

  У нас с некоторыми белками это не работало и мы заменили комнаду на obabel, который использовался при подготовке лиганда (см. 1п.)
 
#### 1.2. Выделение кармана связывания (Pocket Extraction)

Это ключевой этап, в котором используется скрипт **`extract_pocket_by_ligand.py`**.

*   **Что происходит:** Скрипт "вырезает" из полной структуры белка только те атомы и аминокислотные остатки, которые находятся в непосредственной близости от референсного лиганда.
*   **Как это работает (механизм `extract_pocket_by_ligand.py`):**
    1.  **Входные данные:** Скрипт принимает на вход:
        *   Путь к PDB-файлу белка, обработанному на шаге 1.1 (`2qbr_reduce.pdb`).
        *   Путь к SDF-файлу референсного лиганда (`2qbr_docked.sdf`).
    2.  **Идентификация лиганда:** Скрипт загружает обе структуры с помощью библиотеки **RDKit**.
    3.  **Выделение окружения:** Используя координаты атомов референсного лиганда, скрипт определяет все атомы белка, находящиеся в радиусе **10 Å** (ангстрем) от лиганда. Эта операция выполняется функцией `ExtractPocketAndLigand` из библиотеки `oddt`, которая является надстройкой над RDKit.
    4.  **Удаление референсного лиганда (опционально):** Важный момент — скрипт может удалить исходный референсный лиганд из структуры белка (`rm_ccd=True`), чтобы в кармане не было "лишних" молекул перед докингом нового лиганда.
    5.  **Сохранение кофакторов:** Скрипт спроектирован так, чтобы сохранить важные кофакторы (например, ионы металлов Zn, Mg), если они попадают в 10 Å радиус.
*   **Результат:** Компактный PDB-файл (`2qbr_pocket.pdb`), содержащий только атомы кармана связывания. Это значительно ускоряет последующие вычисления, так как нейросети не нужно обрабатывать весь белок.

Этот скрипт мы запускали, работал вроде бы без нареканий.

#### Шаг 2: Подготовка Лиганда (Ligand Preparation)

Здесь мы готовим молекулу, которую собираемся "докировать" (встраивать) в белковый карман.

##### 2.1. Протонирование лиганда

*   **Что происходит:** Аналогично белку, для лиганда определяется его наиболее вероятное состояние протонирования при заданном pH (обычно 7.4). Это критически важно для правильного моделирования взаимодействий, особенно водородных связей.
*   **Инструмент:** В README используется **OpenBabel (`obabel`)**.
    ```bash
    # Команда из README
    obabel examples/raw/2qbr_ligand.sdf -p 7.4 -O examples/ligand/2qbr_docked.sdf
    ```
*   **Результат:** Новый SDF-файл (`2qbr_docked.sdf`) с лигандом, у которого добавлены водороды и выставлены правильные заряды.

#### 2.2. Генерация начальной 3D-конформации

Это второй ключевой этап, где используется скрипт **`rdkit_ETKDG_3d_gen.py`**.

*   **Как скрипт работает:**
    1.  **Входные данные:** Скрипт берет SDF-файл лиганда, полученный на шаге 2.1.
    2.  **Генерация множества конформаций:** С помощью алгоритма **ETKDGv3** в RDKit генерируется несколько (в коде `n_confs=30`) возможных 3D-структур молекулы. Этот алгоритм хорошо воссоздает правильную геометрию молекулы.
    3.  **Минимизация энергии:** Каждая из сгенерированных конформаций оптимизируется с помощью силового поля **UFF (Universal Force Field)**. Этот процесс похож на "встряхивание" молекулы, чтобы ее атомы заняли наиболее стабильное, низкоэнергетическое положение.
    4.  **Выбор лучшей конформации:** Из всех оптимизированных конформаций выбирается одна — та, у которой самая низкая энергия.
*   **Результат:** Новый SDF-файл (`2qbr_uff.sdf`), содержащий единственную, энергетически выгодную 3D-структуру лиганда. Это и есть стартовая поза для докинга.

  На основе данного скрипта написан наш **```prepare_ligands.py```**, но с обработкой ошибок, выводом их и с фильтрацией по молекулярной массе.

#### Шаг 3: Финальная структура папок и запуск докинга

После выполнения всех шагов, у нас есть готовые файлы, которые Interformer ожидает найти в определенной структуре папок:

*   `examples/pocket/2qbr_pocket.pdb`: Вырезанный и обработанный **карман белка**.
*   `examples/ligand/2qbr_docked.sdf`: Протонированный **референсный лиганд**. Он используется для определения центра докинга.
*   `examples/uff/2qbr_uff.sdf`: Низкоэнергетическая **3D-структура лиганда**, которую мы будем докировать.

### 3. Установка
Скрипт со свежей успешной установкой - installation.sh.

### 4. Отладка ошибок
Для большинства систем ошибок возникало немного, и мы вручную пару раз перезапускали одну и ту же симуляцию, удалив лиганд, на котором всё ломалось. Самые большие проблемы были при работе с 1tqn_ic40. Для него в итоговой версии написан отдельный bash-скрипт для запуска симуляции **```pipeline_mmff_uff.sh```** , для подготовки лигандов - **```prepare_ligands_mmff_uff.py```**. Изначально ошибку пытались исправить не меняя оригинальные скрипты Interformer, поэтому для некоторых лигандов из данной системы, при неудачной попытке оптимизации с помощью uff поля, применялось более универсальное mmff, а также были удалены все серосодержащие лиганды (их было немного), так как в логах ошибки явно указывали на некоторые из них. Проблемные молекулы сохранены в result/trouble_ligands_{pdb_id}.csv. В остальном скрипты идентичны тем, что использовались для всех остальных систем. 

Также нам пришлось добавить небольшую отладку в один их оригинальных скриптов. В скрипте ```docking/pdbqt_ligand/wrappers_for_third_party_tools/wrapper_rdkitmol
/wrapper_rdkitmol.py``` изменена только одна функция — классовый метод `save_sdf_given_wrappers`. 

Основное и единственное изменение заключается в добавлении логики, которая **перенаправляет сохранение файлов из временной директории `/tmp/` в другую, постоянную директорию.**

Давайте разберем, как это работает:

**Оригинальная версия (что было):**

```python
# ...
else:
    # rewrite the sdf
    io_writer = open(abspath_sdf_to_save, 'w')
# ...
```

В исходном коде, если файл не дописывается (`append2sdf` равно `False`), он просто создается или перезаписывается по тому пути, который был передан в функцию (`abspath_sdf_to_save`).

---

**Измененная версия (что стало):**

```python
# ...
else:
    # rewrite the sdf
    if abspath_sdf_to_save.startswith('/tmp/'):
        import os
        filename = os.path.basename(abspath_sdf_to_save)
        new_path_dir = '/mnt/tank/scratch/ikarpushkina/Interformer/tmp_output'
        abspath_sdf_to_save = os.path.join(new_path_dir, filename)
    io_writer = open(abspath_sdf_to_save, 'w')
# ...
```

Теперь, перед созданием файла, скрипт выполняет проверку:

1.  **Условие:** `if abspath_sdf_to_save.startswith('/tmp/')`
    *   Проверяется, начинается ли путь для сохранения файла с `/tmp/`. Эта директория в Linux-системах обычно используется для временных файлов, которые могут удаляться после перезагрузки.

2.  **Действие (если условие истинно):**
    *   Из исходного пути извлекается только имя файла (например, из `/tmp/output_pose.sdf` будет взято `output_pose.sdf`).
    *   Задается новый, "жестко" прописанный в коде путь к директории: `/mnt/tank/scratch/ikarpushkina/Interformer/tmp_output`.
    *   Формируется новый полный путь для сохранения, который теперь ведет в эту постоянную директорию (например, `/mnt/tank/scratch/ikarpushkina/Interformer/tmp_output/output_pose.sdf`).

3.  **Результат:** Файл, который изначально должен был сохраниться во временной папке `/tmp`, будет принудительно сохранен в другом, постоянном месте. Если же исходный путь не начинается с `/tmp/`, функция работает как и раньше. 

Такое изменение было нужно нам для отладки.


# Новые изменения в работу основного пайплайна в сравнении с оригинальной задумкой:

Мы реализовали подход, который залючается в многократном запуске с подменой контекста, что необходимо для устойчивой работы инструмента. 

Это кардинально отличается от того, как авторы Interformer задумывали использование своего инструмента. Ниже подробное сравнение.

---

### 1. Канонический подход
*Описан в README и используется в файлах типа `inference.py` и `pipeline.sh`.*

**Суть:** **Пакетная обработка (Batch Processing).**
Авторы предполагают, что вы готовите один большой CSV-файл со всеми лигандами и скармливаете его нейросети.

*   **Идентификация:** Система опирается на **PDB ID** (первые 4 символа имени файла или колонки Target).
*   **Кэширование:** Чтобы ускорить работу, Interformer агрессивно кэширует предобработанные данные (графы, фичи) в папку `tmp_beta`. Кэш привязывается к имени файла и PDB ID.
*   **Поток данных:**
    1.  `inference.py` (Energy) читает весь CSV, считает энергии для всех строк, сохраняет результаты.
    2.  `reconstruct_ligands.py` (C++) берет папку с энергиями и восстанавливает позы для всей пачки.
    3.  `inference.py` (Affinity) оценивает всю пачку поз разом.
*   **Минусы:**
    *   **Хрупкость:** Если C++ код (докинг) упадет на одном сложном лиганде (segfault), весь процесс остановится.
    *   **Конфликты файлов:** Скрипты часто перезаписывают файлы с одинаковыми именами (например, `pocket.pdb`), если не следить за структурой папок идеально.
    *   **"Утечка данных" при циклах:** Если попытаться запустить этот процесс в цикле `for` для одного белка, кэш `tmp_beta` не обновляется, и для нового лиганда выдаются результаты старого (что мы и наблюдали).

---

### 2. Наш подход (Implemented Robust Pipeline)
*Реализован в финальном `master_pipeline_log.py`.*

**Суть:** **Итеративная изоляция (Iterative Isolation).**
Мы обманываем Interformer, заставляя его думать, что каждый лиганд — это **совершенно новый, уникальный проект с новым белком**, который он видит впервые.

#### Ключевые отличия:

| Характеристика | Канонический Interformer | Наш подход (Master Pipeline) |
| :--- | :--- | :--- |
| **Режим запуска** | Один запуск на ~1000 лигандов (Batch). | ~1000 запусков по 1 лиганду (Loop). |
| **Идентификация** | Использует реальный PDB ID (например, `1tqn`). | **Генерирует фейковый ID** (`L001`, `L002`...) для каждого лиганда. |
| **Кэширование** | Активно использует `tmp_beta` для ускорения. | **Жестко удаляет** `tmp_beta` перед каждым шагом + фейковый ID гарантирует уникальность кэша. |
| **Файловая система** | Все результаты валятся в одну кучу (`energy_output`). | Каждый лиганд работает в **изолированной песочнице** (`temp_isolation_work/L001`), которая удаляется после успеха. |
| **Устойчивость** | Ошибка на одном лиганде крашит всё. | Ошибка ловится (`try-except`), записывается в отчет, скрипт переходит к следующему. |
| **Время выполнения** | Быстрее (меньше накладных расходов на запуск Python). | Чуть медленнее (перезапуск среды для каждого лиганда), но **гарантирует результат**. |

#### Как это работает пошагово (в нашем скрипте):

1.  **Подготовка:** Мы берем реальный белок `1tqn` и сохраняем его копию.
2.  **Цикл по лигандам:** Берем очередной лиганд.
3.  **Подмена личности (Spoofing):**
    *   Присваиваем ему ID `L001`.
    *   Создаем папку `temp/L001`.
    *   Копируем туда белок, но переименовываем его в `L001_pocket.pdb`.
    *   Копируем лиганд как `L001_uff.sdf`.
4.  **Запуск:** Запускаем Interformer. Он видит "белок L001", не находит его в кэше (так как такого белка не существует) и честно проводит все вычисления с нуля.
5.  **Сбор урожая:** Забираем результаты из папки `L001`, меняем в CSV имя обратно на `1tqn` и сохраняем в чистовик.
6.  **Зачистка:** Удаляем папку `L001` и кэш, чтобы следующий лиганд (`L002`) начал с чистого листа.

### Почему мы к этому пришли?

Мы столкнулись с архитектурной особенностью Interformer: он **полагается на кэширование и имена файлов**. При попытке сделать "устойчивый цикл" стандартными средствами, нейросеть просто отдавала нам старые результаты (Data Leakage), потому что видела знакомое имя файла (`1tqn_...csv`).

Подход с **фейковыми ID (`L001`)** — это способ гарантированно обойти внутренние механизмы кэширования Interformer без переписывания его исходного кода. Это делает процесс немного "тяжелее", но зато надежным и воспроизводимым.
